{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm  # Use tqdm.notebook for Jupyter\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# For inline plots\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find unique labels\n",
    "def find_unique_labels(filename):\n",
    "    # Make an empty set\n",
    "    unique_labels = set()\n",
    "    with open(filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    for line in lines:\n",
    "        unique_labels.add(line.strip())\n",
    "    return unique_labels\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_data(sequences_file, labels_file):\n",
    "    # Load sequences\n",
    "    with open(sequences_file, 'r') as file:\n",
    "        sequences = [line.strip() for line in file.readlines()]\n",
    "    \n",
    "    # Load labels\n",
    "    with open(labels_file, 'r') as file:\n",
    "        labels = [line.strip() for line in file.readlines()]\n",
    "    \n",
    "    # Create a mapping from label text to numerical value\n",
    "    unique_labels = list(set(labels))\n",
    "    label_to_id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    \n",
    "    # Convert label text to numerical values\n",
    "    label_ids = [label_to_id[label] for label in labels]\n",
    "    \n",
    "    print(f\"Loaded {len(sequences)} sequences with average length: {sum(len(s) for s in sequences)/len(sequences):.1f}\")\n",
    "    print(f\"Found {len(unique_labels)} unique labels: {', '.join(unique_labels)}\")\n",
    "    \n",
    "    return sequences, label_ids, label_to_id, unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SequenceDataset class\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, k=6, max_length=512, stride=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.k = k  # k-mer size\n",
    "        self.stride = stride  # for overlapping chunks\n",
    "        \n",
    "        # Pre-process sequences to k-mers\n",
    "        self.processed_texts = []\n",
    "        self.processed_labels = []\n",
    "        self._preprocess_sequences()\n",
    "        \n",
    "    def _create_kmers(self, sequence):\n",
    "        \"\"\"Convert a sequence to k-mers\"\"\"\n",
    "        return [sequence[i:i+self.k] for i in range(len(sequence) - self.k + 1)]\n",
    "    \n",
    "    def _preprocess_sequences(self):\n",
    "        \"\"\"Convert sequences to k-mers and handle chunking for long sequences\"\"\"\n",
    "        for idx, (seq, label) in enumerate(zip(self.texts, self.labels)):\n",
    "            # Generate k-mers for the sequence\n",
    "            kmers = self._create_kmers(seq)\n",
    "            \n",
    "            # Join k-mers with spaces to create a \"sentence\" BERT can process\n",
    "            kmer_text = \" \".join(kmers)\n",
    "            \n",
    "            # Tokenize the k-mer text\n",
    "            tokens = self.tokenizer.tokenize(kmer_text)\n",
    "            \n",
    "            # If the tokenized sequence is shorter than max_length, add it directly\n",
    "            if len(tokens) <= self.max_length - 2:  # -2 for [CLS] and [SEP]\n",
    "                self.processed_texts.append(kmer_text)\n",
    "                self.processed_labels.append(label)\n",
    "            else:\n",
    "                # For longer sequences, we need to chunk them with overlap\n",
    "                # The tokenizer will add the [CLS] and [SEP] tokens for each chunk\n",
    "                text_chunks = []\n",
    "                \n",
    "                # Split into chunks with overlap\n",
    "                chunk_length = self.max_length - 2  # -2 for [CLS] and [SEP]\n",
    "                for i in range(0, len(tokens), self.stride):\n",
    "                    chunk = tokens[i:i + chunk_length]\n",
    "                    if len(chunk) > chunk_length // 2:  # Ensure chunk is reasonably sized\n",
    "                        text_chunks.append(self.tokenizer.convert_tokens_to_string(chunk))\n",
    "                \n",
    "                # Add each chunk as a separate entry with the same label\n",
    "                for chunk in text_chunks:\n",
    "                    self.processed_texts.append(chunk)\n",
    "                    self.processed_labels.append(label)\n",
    "        \n",
    "        print(f\"Processed {len(self.texts)} sequences into {len(self.processed_texts)} chunks\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.processed_texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.processed_texts[idx]\n",
    "        label = self.processed_labels[idx]\n",
    "        \n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Remove the batch dimension added by the tokenizer\n",
    "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "        \n",
    "        # Add the label\n",
    "        encoding['label'] = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        return encoding\n",
    "    \n",
    "# Create specialized dataset for evaluation with chunk tracking\n",
    "class ChunkTrackingDataset(SequenceDataset):\n",
    "    def __init__(self, texts, labels, tokenizer, k=6, max_length=512, stride=256):\n",
    "        # Store original sequence count before chunking\n",
    "        self.original_count = len(texts)\n",
    "        \n",
    "        # Call parent init to do chunking\n",
    "        super().__init__(texts, labels, tokenizer, k, max_length, stride)\n",
    "        \n",
    "        # Create a mapping from chunk to original sequence\n",
    "        self.chunk_to_original = []\n",
    "        \n",
    "        # Track which chunks belong to which original sequence\n",
    "        chunk_idx = 0\n",
    "        for idx, seq in enumerate(texts):\n",
    "            # Generate k-mers for the sequence\n",
    "            kmers = self._create_kmers(seq)\n",
    "            kmer_text = \" \".join(kmers)\n",
    "            tokens = self.tokenizer.tokenize(kmer_text)\n",
    "            \n",
    "            if len(tokens) <= self.max_length - 2:\n",
    "                # One chunk for this sequence\n",
    "                self.chunk_to_original.append(idx)\n",
    "                chunk_idx += 1\n",
    "            else:\n",
    "                # Multiple chunks for this sequence\n",
    "                chunk_length = self.max_length - 2\n",
    "                for i in range(0, len(tokens), self.stride):\n",
    "                    chunk = tokens[i:i + chunk_length]\n",
    "                    if len(chunk) > chunk_length // 2:\n",
    "                        self.chunk_to_original.append(idx)\n",
    "                        chunk_idx += 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get the base encoding from parent class\n",
    "        encoding = super().__getitem__(idx)\n",
    "        \n",
    "        # Add original sequence index\n",
    "        encoding['original_index'] = torch.tensor(self.chunk_to_original[idx], dtype=torch.long)\n",
    "        \n",
    "        return encoding\n",
    "\n",
    "# %%\n",
    "# Model definition: BERT with linear regression head for classification\n",
    "class BERTSequenceClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, freeze_bert=True):\n",
    "        super(BERTSequenceClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        # Linear regression head\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)  # Add dropout for regularization\n",
    "        \n",
    "        # Freeze BERT parameters if specified\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        # Get BERT outputs\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        # We take the [CLS] token representation (first token)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)  # Apply dropout\n",
    "        \n",
    "        # Pass through the linear regression head\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits\n",
    "        \n",
    "# Model for chunk-based prediction aggregation\n",
    "class ChunkAggregationModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(ChunkAggregationModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        return self.base_model(input_ids, attention_mask, token_type_ids)\n",
    "    \n",
    "    def predict_with_chunks(self, chunks_dataloader, device, original_sequence_count):\n",
    "        \"\"\"\n",
    "        Predict using multiple chunks per sequence and aggregate results.\n",
    "        \n",
    "        Args:\n",
    "            chunks_dataloader: DataLoader containing chunks of sequences\n",
    "            device: Device to run inference on\n",
    "            original_sequence_count: Number of original sequences before chunking\n",
    "            \n",
    "        Returns:\n",
    "            List of predictions for each original sequence\n",
    "        \"\"\"\n",
    "        self.base_model.eval()\n",
    "        \n",
    "        all_logits = []\n",
    "        all_chunk_indices = []\n",
    "        \n",
    "        # First, get logits for all chunks\n",
    "        with torch.no_grad():\n",
    "            for batch in chunks_dataloader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                chunk_indices = batch['original_index'].to(device)\n",
    "                \n",
    "                logits = self.base_model(input_ids, attention_mask)\n",
    "                \n",
    "                all_logits.append(logits.cpu())\n",
    "                all_chunk_indices.append(chunk_indices.cpu())\n",
    "        \n",
    "        # Concatenate results\n",
    "        all_logits = torch.cat(all_logits, dim=0)\n",
    "        all_chunk_indices = torch.cat(all_chunk_indices, dim=0)\n",
    "        \n",
    "        # Aggregate predictions for each original sequence\n",
    "        final_predictions = []\n",
    "        for i in range(original_sequence_count):\n",
    "            # Get logits for all chunks of this sequence\n",
    "            mask = (all_chunk_indices == i)\n",
    "            sequence_logits = all_logits[mask]\n",
    "            \n",
    "            if len(sequence_logits) == 0:\n",
    "                # Fallback if no chunks for this sequence (shouldn't happen normally)\n",
    "                final_predictions.append(torch.zeros(all_logits.shape[1]).argmax().item())\n",
    "                continue\n",
    "                \n",
    "            # Average logits across chunks\n",
    "            avg_logits = torch.mean(sequence_logits, dim=0)\n",
    "            \n",
    "            # Get predicted class\n",
    "            prediction = avg_logits.argmax().item()\n",
    "            final_predictions.append(prediction)\n",
    "        \n",
    "        return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Apple Silicon) acceleration\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(true_labels, pred_labels, label_names, original_sequences=None, predicted_sequences=None):\n",
    "    # Print classification report\n",
    "    report = classification_report(true_labels, pred_labels, target_names=label_names, digits=4)\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = np.zeros((len(label_names), len(label_names)), dtype=int)\n",
    "    for t, p in zip(true_labels, pred_labels):\n",
    "        cm[t][p] += 1\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_names, yticklabels=label_names)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()  # Display directly in notebook\n",
    "    \n",
    "    # Calculate BLEU and ROUGE if sequences are provided\n",
    "    if original_sequences is not None and predicted_sequences is not None:\n",
    "        calculate_sequence_metrics(original_sequences, predicted_sequences)\n",
    "\n",
    "# Function to calculate BLEU and ROUGE scores for sequences\n",
    "def calculate_sequence_metrics(original_sequences, predicted_sequences):\n",
    "    # Prepare for BLEU calculation\n",
    "    smooth = SmoothingFunction().method1\n",
    "    bleu_scores = []\n",
    "    \n",
    "    # Initialize ROUGE scorer\n",
    "    rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "    \n",
    "    # Calculate scores for each pair of sequences\n",
    "    for orig, pred in zip(original_sequences, predicted_sequences):\n",
    "        # For BLEU score, convert sequences to list of characters\n",
    "        orig_tokens = list(orig)\n",
    "        pred_tokens = list(pred)\n",
    "        \n",
    "        # Calculate BLEU - using character-level tokenization\n",
    "        try:\n",
    "            bleu_score = sentence_bleu([orig_tokens], pred_tokens, smoothing_function=smooth)\n",
    "            bleu_scores.append(bleu_score)\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating BLEU: {e}\")\n",
    "            \n",
    "        # Calculate ROUGE scores\n",
    "        try:\n",
    "            rouge_result = rouge_scorer_instance.score(orig, pred)\n",
    "            for metric, score in rouge_result.items():\n",
    "                rouge_scores[metric].append(score.fmeasure)\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating ROUGE: {e}\")\n",
    "    \n",
    "    # Print average BLEU score\n",
    "    if bleu_scores:\n",
    "        print(f\"\\nAverage BLEU score: {np.mean(bleu_scores):.4f}\")\n",
    "    \n",
    "    # Print average ROUGE scores\n",
    "    if all(scores for scores in rouge_scores.values()):\n",
    "        print(\"\\nAverage ROUGE scores:\")\n",
    "        for metric, scores in rouge_scores.items():\n",
    "            if scores:\n",
    "                print(f\"  {metric}: {np.mean(scores):.4f}\")\n",
    "                \n",
    "    # Plot histogram of BLEU scores\n",
    "    if bleu_scores:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(bleu_scores, bins=20, alpha=0.7)\n",
    "        plt.xlabel('BLEU Score')\n",
    "        plt.ylabel('Number of Sequences')\n",
    "        plt.title('Distribution of BLEU Scores')\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.show()\n",
    "    \n",
    "    # Plot histogram of ROUGE scores\n",
    "    if all(scores for scores in rouge_scores.values()):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        for i, (metric, scores) in enumerate(rouge_scores.items()):\n",
    "            if scores:\n",
    "                plt.subplot(1, 3, i+1)\n",
    "                plt.hist(scores, bins=20, alpha=0.7)\n",
    "                plt.xlabel(f'{metric} Score')\n",
    "                plt.ylabel('Number of Sequences')\n",
    "                plt.title(f'Distribution of {metric} Scores')\n",
    "                plt.grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Function to get actual sequences for predictions\n",
    "def get_sequences_for_predictions(val_sequences, true_labels, pred_labels, id_to_label):\n",
    "    \"\"\"\n",
    "    Get actual sequences for both true and predicted labels\n",
    "    Args:\n",
    "        val_sequences: Original validation sequences\n",
    "        true_labels: Numerical IDs for true labels\n",
    "        pred_labels: Numerical IDs for predicted labels\n",
    "        id_to_label: Mapping from ID to label name\n",
    "    \n",
    "    Returns:\n",
    "        List of original sequences for each true and predicted label\n",
    "    \"\"\"\n",
    "    true_sequences = []\n",
    "    pred_sequences = []\n",
    "    \n",
    "    # Convert numerical labels to actual labels\n",
    "    true_label_names = [id_to_label[label_id] for label_id in true_labels]\n",
    "    pred_label_names = [id_to_label[label_id] for label_id in pred_labels]\n",
    "    \n",
    "    # For demonstration - typically you'd match specific sequences to predictions\n",
    "    # This is a simplified approach assuming the labels represent different sequence types\n",
    "    for i, sequence in enumerate(val_sequences):\n",
    "        # Add original sequence for both lists (in practice, you'd have true sequences)\n",
    "        true_sequences.append(sequence)\n",
    "        \n",
    "        # For predicted sequences - in practice, you might have a sequence generation model\n",
    "        # Here we just use original sequence as a placeholder\n",
    "        pred_sequences.append(sequence) \n",
    "    \n",
    "    return true_sequences, pred_sequences\n",
    "\n",
    "# Function to visualize training progress\n",
    "def plot_training_progress(train_losses, val_losses, val_accuracies):\n",
    "    # Plot losses\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()  # Display directly in notebook\n",
    "\n",
    "# %%\n",
    "# Set paths\n",
    "sequences_file = \"extracted-spike-data/Spike7k_sequences.txt\"\n",
    "labels_file = \"extracted-spike-data/Spike7k_labels.txt\"\n",
    "\n",
    "# Hyperparameters\n",
    "k = 6                    # k-mer size\n",
    "max_length = 512         # BERT's max token length\n",
    "stride = 256             # Stride for chunking overlaps\n",
    "batch_size = 16          # Batch size for training\n",
    "num_epochs = 5           # Number of training epochs\n",
    "learning_rate = 2e-5     # Learning rate for optimizer\n",
    "freeze_bert = True       # Whether to freeze BERT parameters\n",
    "\n",
    "# Set device - with MPS support for Apple Silicon\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple Silicon) acceleration\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA acceleration\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU (no GPU acceleration available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'extracted-spike-data/Spike7k_sequences.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m sequences, label_ids, label_to_id, unique_labels \u001b[38;5;241m=\u001b[39m load_data(sequences_file, labels_file)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Create inverse mapping from ID to label\u001b[39;00m\n\u001b[1;32m      6\u001b[0m id_to_label \u001b[38;5;241m=\u001b[39m {idx: label \u001b[38;5;28;01mfor\u001b[39;00m label, idx \u001b[38;5;129;01min\u001b[39;00m label_to_id\u001b[38;5;241m.\u001b[39mitems()}\n",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(sequences_file, labels_file)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(sequences_file, labels_file):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Load sequences\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(sequences_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     15\u001b[0m         sequences \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file\u001b[38;5;241m.\u001b[39mreadlines()]\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Load labels\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'extracted-spike-data/Spike7k_sequences.txt'"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Load data\n",
    "sequences, label_ids, label_to_id, unique_labels = load_data(sequences_file, labels_file)\n",
    "\n",
    "# Create inverse mapping from ID to label\n",
    "id_to_label = {idx: label for label, idx in label_to_id.items()}\n",
    "\n",
    "# Split data\n",
    "train_sequences, val_sequences, train_labels, val_labels = train_test_split(\n",
    "    sequences, label_ids, test_size=0.2, random_state=25, stratify=label_ids\n",
    ")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create datasets with k-mer processing\n",
    "train_dataset = SequenceDataset(train_sequences, train_labels, tokenizer, \n",
    "                               k=k, max_length=max_length, stride=stride)\n",
    "\n",
    "# Use the chunk tracking dataset for validation to enable proper aggregation\n",
    "val_dataset = ChunkTrackingDataset(val_sequences, val_labels, tokenizer,\n",
    "                                 k=k, max_length=max_length, stride=stride)\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Initialize model with frozen BERT if specified\n",
    "base_model = BERTSequenceClassifier(num_classes=len(unique_labels), freeze_bert=freeze_bert)\n",
    "base_model.to(device)\n",
    "\n",
    "# Initialize optimizer - only optimize parameters that require gradients\n",
    "optimizer = AdamW([p for p in base_model.parameters() if p.requires_grad], lr=learning_rate)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training history\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Create checkpoint directory\n",
    "checkpoint_dir = 'checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Initialize checkpoint variables\n",
    "best_accuracy = 0.0\n",
    "start_epoch = 0\n",
    "\n",
    "# Check if checkpoint exists to resume training\n",
    "checkpoint_path = os.path.join(checkpoint_dir, 'latest_checkpoint.pt')\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    base_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_accuracy = checkpoint['best_accuracy']\n",
    "    if 'train_losses' in checkpoint:\n",
    "        train_losses = checkpoint['train_losses']\n",
    "        val_losses = checkpoint['val_losses']\n",
    "        val_accuracies = checkpoint['val_accuracies']\n",
    "    print(f\"Resuming from epoch {start_epoch} with best accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    try:\n",
    "        # Training phase\n",
    "        base_model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_dataloader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase - use chunk aggregation\n",
    "        base_model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        # Create aggregation model\n",
    "        chunk_model = ChunkAggregationModel(base_model)\n",
    "        \n",
    "        # Get aggregated predictions\n",
    "        val_preds = chunk_model.predict_with_chunks(val_dataloader, device, len(val_sequences))\n",
    "        val_true = val_labels  # Original labels\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(val_true, val_preds)\n",
    "        val_accuracies.append(accuracy)\n",
    "        \n",
    "        # For validation loss, we need to iterate through batches\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dataloader, desc=f\"Validation Epoch {epoch+1}\"):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                outputs = base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"  Val Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        # Create checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': base_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'val_accuracies': val_accuracies,\n",
    "            'best_accuracy': best_accuracy,\n",
    "            'config': {\n",
    "                'k': k,\n",
    "                'max_length': max_length,\n",
    "                'stride': stride,\n",
    "                'freeze_bert': freeze_bert,\n",
    "                'batch_size': batch_size,\n",
    "                'learning_rate': learning_rate\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save the latest checkpoint (overwrite previous)\n",
    "        torch.save(checkpoint, os.path.join(checkpoint_dir, 'latest_checkpoint.pt'))\n",
    "        \n",
    "        # Save epoch-specific checkpoint\n",
    "        torch.save(checkpoint, os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pt'))\n",
    "        \n",
    "        # Save best model\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(checkpoint, os.path.join(checkpoint_dir, 'best_model.pt'))\n",
    "            print(f\"  New best model saved with accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted by user. Saving checkpoint...\")\n",
    "        # Save interrupt checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': base_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'val_accuracies': val_accuracies,\n",
    "            'best_accuracy': best_accuracy,\n",
    "            'config': {\n",
    "                'k': k,\n",
    "                'max_length': max_length,\n",
    "                'stride': stride,\n",
    "                'freeze_bert': freeze_bert\n",
    "            }\n",
    "        }\n",
    "        torch.save(checkpoint, os.path.join(checkpoint_dir, 'interrupt_checkpoint.pt'))\n",
    "        print(f\"Interrupt checkpoint saved. Resume with checkpoint_path='interrupt_checkpoint.pt'\")\n",
    "        break\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during training: {str(e)}\")\n",
    "        # Save emergency checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': base_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'val_accuracies': val_accuracies,\n",
    "            'best_accuracy': best_accuracy\n",
    "        }\n",
    "        torch.save(checkpoint, os.path.join(checkpoint_dir, f'emergency_checkpoint_epoch_{epoch+1}.pt'))\n",
    "        print(f\"Emergency checkpoint saved to 'emergency_checkpoint_epoch_{epoch+1}.pt'\")\n",
    "        raise  # Re-raise the exception for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Get sequences for evaluation\n",
    "true_sequences, pred_sequences = get_sequences_for_predictions(\n",
    "    val_sequences, val_true, val_preds, id_to_label\n",
    ")\n",
    "\n",
    "# Evaluate model with BLEU and ROUGE scores\n",
    "evaluate_model(val_true, val_preds, unique_labels, true_sequences, pred_sequences)\n",
    "\n",
    "# Plot training progress\n",
    "plot_training_progress(train_losses, val_losses, val_accuracies)\n",
    "\n",
    "# Save model\n",
    "torch.save(base_model.state_dict(), 'bert_sequence_classifier.pt')\n",
    "\n",
    "# Save k-mer and tokenization parameters\n",
    "config = {\n",
    "    'k': k,\n",
    "    'max_length': max_length,\n",
    "    'stride': stride,\n",
    "    'num_classes': len(unique_labels),\n",
    "    'freeze_bert': freeze_bert\n",
    "}\n",
    "\n",
    "with open('model_config.json', 'w') as f:\n",
    "    json.dump(config, f)\n",
    "\n",
    "# Save label mapping\n",
    "with open('label_mapping.txt', 'w') as f:\n",
    "    for label, idx in label_to_id.items():\n",
    "        f.write(f\"{label}\\t{idx}\\n\")\n",
    "\n",
    "print(\"Training completed and model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Generate detailed BLEU and ROUGE evaluation report\n",
    "def generate_evaluation_report(true_sequences, pred_sequences, true_labels, pred_labels, id_to_label):\n",
    "    \"\"\"\n",
    "    Generate a detailed evaluation report with per-class BLEU and ROUGE metrics\n",
    "    \"\"\"\n",
    "    # Get label names\n",
    "    true_label_names = [id_to_label[label_id] for label_id in true_labels]\n",
    "    pred_label_names = [id_to_label[label_id] for label_id in pred_labels]\n",
    "    \n",
    "    # Group sequences by class\n",
    "    classes = {}\n",
    "    for i, (true_seq, pred_seq, true_label, pred_label) in enumerate(zip(\n",
    "            true_sequences, pred_sequences, true_label_names, pred_label_names)):\n",
    "        \n",
    "        # Initialize class if not seen before\n",
    "        if true_label not in classes:\n",
    "            classes[true_label] = {\n",
    "                'count': 0,\n",
    "                'correct': 0,\n",
    "                'bleu_scores': [],\n",
    "                'rouge1_scores': [],\n",
    "                'rouge2_scores': [],\n",
    "                'rougeL_scores': []\n",
    "            }\n",
    "        \n",
    "        # Update class stats\n",
    "        classes[true_label]['count'] += 1\n",
    "        if true_label == pred_label:\n",
    "            classes[true_label]['correct'] += 1\n",
    "        \n",
    "        # Calculate BLEU\n",
    "        try:\n",
    "            smooth = SmoothingFunction().method1\n",
    "            true_tokens = list(true_seq)\n",
    "            pred_tokens = list(pred_seq)\n",
    "            bleu = sentence_bleu([true_tokens], pred_tokens, smoothing_function=smooth)\n",
    "            classes[true_label]['bleu_scores'].append(bleu)\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating BLEU for sequence {i}: {e}\")\n",
    "        \n",
    "        # Calculate ROUGE\n",
    "        try:\n",
    "            scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "            scores = scorer.score(true_seq, pred_seq)\n",
    "            classes[true_label]['rouge1_scores'].append(scores['rouge1'].fmeasure)\n",
    "            classes[true_label]['rouge2_scores'].append(scores['rouge2'].fmeasure)\n",
    "            classes[true_label]['rougeL_scores'].append(scores['rougeL'].fmeasure)\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating ROUGE for sequence {i}: {e}\")\n",
    "    \n",
    "    # Print report\n",
    "    print(\"\\n===== Detailed Evaluation Report =====\")\n",
    "    print(f\"Total sequences: {len(true_sequences)}\")\n",
    "    \n",
    "    # Overall metrics\n",
    "    all_bleu = [score for class_data in classes.values() for score in class_data['bleu_scores']]\n",
    "    all_rouge1 = [score for class_data in classes.values() for score in class_data['rouge1_scores']]\n",
    "    all_rouge2 = [score for class_data in classes.values() for score in class_data['rouge2_scores']]\n",
    "    all_rougeL = [score for class_data in classes.values() for score in class_data['rougeL_scores']]\n",
    "    \n",
    "    print(\"\\nOverall Metrics:\")\n",
    "    print(f\"  Average BLEU: {np.mean(all_bleu):.4f}\")\n",
    "    print(f\"  Average ROUGE-1: {np.mean(all_rouge1):.4f}\")\n",
    "    print(f\"  Average ROUGE-2: {np.mean(all_rouge2):.4f}\")\n",
    "    print(f\"  Average ROUGE-L: {np.mean(all_rougeL):.4f}\")\n",
    "    \n",
    "    # Per-class metrics\n",
    "    print(\"\\nPer-Class Metrics:\")\n",
    "    for class_name, data in classes.items():\n",
    "        accuracy = data['correct'] / data['count'] if data['count'] > 0 else 0\n",
    "        print(f\"\\n  Class: {class_name}\")\n",
    "        print(f\"    Count: {data['count']}\")\n",
    "        print(f\"    Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"    Average BLEU: {np.mean(data['bleu_scores']):.4f}\")\n",
    "        print(f\"    Average ROUGE-1: {np.mean(data['rouge1_scores']):.4f}\")\n",
    "        print(f\"    Average ROUGE-2: {np.mean(data['rouge2_scores']):.4f}\")\n",
    "        print(f\"    Average ROUGE-L: {np.mean(data['rougeL_scores']):.4f}\")\n",
    "\n",
    "# Run the detailed evaluation\n",
    "generate_evaluation_report(true_sequences, pred_sequences, val_true, val_preds, id_to_label)\n",
    "\n",
    "# %%\n",
    "# Function to evaluate sequence prediction quality with visualizations\n",
    "def visualize_sequence_predictions(true_sequences, pred_sequences, true_labels, pred_labels, id_to_label, num_examples=5):\n",
    "    \"\"\"\n",
    "    Visualize samples of true vs predicted sequences along with their metrics\n",
    "    \"\"\"\n",
    "    # Convert numerical labels to names\n",
    "    true_label_names = [id_to_label[label_id] for label_id in true_labels]\n",
    "    pred_label_names = [id_to_label[label_id] for label_id in pred_labels]\n",
    "    \n",
    "    # Get correctly and incorrectly classified examples\n",
    "    correct_indices = [i for i, (t, p) in enumerate(zip(true_labels, pred_labels)) if t == p]\n",
    "    incorrect_indices = [i for i, (t, p) in enumerate(zip(true_labels, pred_labels)) if t != p]\n",
    "    \n",
    "    # Initialize ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    smooth = SmoothingFunction().method1\n",
    "    \n",
    "    # Display examples\n",
    "    print(\"\\n===== Example Predictions =====\")\n",
    "    \n",
    "    # Show some correct examples\n",
    "    print(\"\\nCorrectly Classified Examples:\")\n",
    "    sample_correct = np.random.choice(correct_indices, min(num_examples, len(correct_indices)), replace=False)\n",
    "    for idx in sample_correct:\n",
    "        # Calculate metrics\n",
    "        true_seq = true_sequences[idx]\n",
    "        pred_seq = pred_sequences[idx]\n",
    "        \n",
    "        # BLEU score\n",
    "        true_tokens = list(true_seq)\n",
    "        pred_tokens = list(pred_seq)\n",
    "        bleu = sentence_bleu([true_tokens], pred_tokens, smoothing_function=smooth)\n",
    "        \n",
    "        # ROUGE scores\n",
    "        rouge_scores = scorer.score(true_seq, pred_seq)\n",
    "        \n",
    "        # Print example\n",
    "        print(f\"\\nExample {idx}:\")\n",
    "        print(f\"  True Label: {true_label_names[idx]}\")\n",
    "        print(f\"  Predicted Label: {pred_label_names[idx]}\")\n",
    "        print(f\"  BLEU Score: {bleu:.4f}\")\n",
    "        print(f\"  ROUGE-1 F1: {rouge_scores['rouge1'].fmeasure:.4f}\")\n",
    "        print(f\"  ROUGE-L F1: {rouge_scores['rougeL'].fmeasure:.4f}\")\n",
    "        \n",
    "        # Print sequence preview (first 50 chars)\n",
    "        print(f\"  True Sequence: {true_seq[:50]}...\" if len(true_seq) > 50 else f\"  True Sequence: {true_seq}\")\n",
    "        print(f\"  Pred Sequence: {pred_seq[:50]}...\" if len(pred_seq) > 50 else f\"  Pred Sequence: {pred_seq}\")\n",
    "    \n",
    "    # Show some incorrect examples\n",
    "    if incorrect_indices:\n",
    "        print(\"\\nIncorrectly Classified Examples:\")\n",
    "        sample_incorrect = np.random.choice(incorrect_indices, min(num_examples, len(incorrect_indices)), replace=False)\n",
    "        for idx in sample_incorrect:\n",
    "            # Calculate metrics\n",
    "            true_seq = true_sequences[idx]\n",
    "            pred_seq = pred_sequences[idx]\n",
    "            \n",
    "            # BLEU score\n",
    "            true_tokens = list(true_seq)\n",
    "            pred_tokens = list(pred_seq)\n",
    "            bleu = sentence_bleu([true_tokens], pred_tokens, smoothing_function=smooth)\n",
    "            \n",
    "            # ROUGE scores\n",
    "            rouge_scores = scorer.score(true_seq, pred_seq)\n",
    "            \n",
    "            # Print example\n",
    "            print(f\"\\nExample {idx}:\")\n",
    "            print(f\"  True Label: {true_label_names[idx]}\")\n",
    "            print(f\"  Predicted Label: {pred_label_names[idx]}\")\n",
    "            print(f\"  BLEU Score: {bleu:.4f}\")\n",
    "            print(f\"  ROUGE-1 F1: {rouge_scores['rouge1'].fmeasure:.4f}\")\n",
    "            print(f\"  ROUGE-L F1: {rouge_scores['rougeL'].fmeasure:.4f}\")\n",
    "            \n",
    "            # Print sequence preview (first 50 chars)\n",
    "            print(f\"  True Sequence: {true_seq[:50]}...\" if len(true_seq) > 50 else f\"  True Sequence: {true_seq}\")\n",
    "            print(f\"  Pred Sequence: {pred_seq[:50]}...\" if len(pred_seq) > 50 else f\"  Pred Sequence: {pred_seq}\")\n",
    "    else:\n",
    "        print(\"\\nNo incorrectly classified examples found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Function to plot BLEU and ROUGE score distributions by class\n",
    "def plot_metric_distributions_by_class(true_sequences, pred_sequences, true_labels, id_to_label):\n",
    "    \"\"\"\n",
    "    Plot the distribution of BLEU and ROUGE scores for each class\n",
    "    \"\"\"\n",
    "    # Convert numerical labels to names\n",
    "    label_names = [id_to_label[label_id] for label_id in true_labels]\n",
    "    unique_labels = list(set(label_names))\n",
    "    \n",
    "    # Initialize metrics storage\n",
    "    class_metrics = {label: {\n",
    "        'bleu': [],\n",
    "        'rouge1': [],\n",
    "        'rouge2': [],\n",
    "        'rougeL': []\n",
    "    } for label in unique_labels}\n",
    "    \n",
    "    # Calculate metrics for each sequence\n",
    "    smooth = SmoothingFunction().method1\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    for i, (true_seq, pred_seq, label) in enumerate(zip(true_sequences, pred_sequences, label_names)):\n",
    "        # BLEU score\n",
    "        try:\n",
    "            true_tokens = list(true_seq)\n",
    "            pred_tokens = list(pred_seq)\n",
    "            bleu = sentence_bleu([true_tokens], pred_tokens, smoothing_function=smooth)\n",
    "            class_metrics[label]['bleu'].append(bleu)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # ROUGE scores\n",
    "        try:\n",
    "            scores = scorer.score(true_seq, pred_seq)\n",
    "            class_metrics[label]['rouge1'].append(scores['rouge1'].fmeasure)\n",
    "            class_metrics[label]['rouge2'].append(scores['rouge2'].fmeasure)\n",
    "            class_metrics[label]['rougeL'].append(scores['rougeL'].fmeasure)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Plot BLEU score distributions\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.title('BLEU Score Distribution by Class')\n",
    "    \n",
    "    # Create box plots for BLEU scores\n",
    "    bleu_data = [class_metrics[label]['bleu'] for label in unique_labels]\n",
    "    plt.boxplot(bleu_data, labels=unique_labels)\n",
    "    plt.ylabel('BLEU Score')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot ROUGE score distributions\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # ROUGE-1\n",
    "    axes[0].set_title('ROUGE-1 F1 Distribution by Class')\n",
    "    rouge1_data = [class_metrics[label]['rouge1'] for label in unique_labels]\n",
    "    axes[0].boxplot(rouge1_data, labels=unique_labels)\n",
    "    axes[0].set_ylabel('ROUGE-1 F1 Score')\n",
    "    axes[0].set_xticklabels(unique_labels, rotation=45, ha='right')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # ROUGE-2\n",
    "    axes[1].set_title('ROUGE-2 F1 Distribution by Class')\n",
    "    rouge2_data = [class_metrics[label]['rouge2'] for label in unique_labels]\n",
    "    axes[1].boxplot(rouge2_data, labels=unique_labels)\n",
    "    axes[1].set_ylabel('ROUGE-2 F1 Score')\n",
    "    axes[1].set_xticklabels(unique_labels, rotation=45, ha='right')\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # ROUGE-L\n",
    "    axes[2].set_title('ROUGE-L F1 Distribution by Class')\n",
    "    rougeL_data = [class_metrics[label]['rougeL'] for label in unique_labels]\n",
    "    axes[2].boxplot(rougeL_data, labels=unique_labels)\n",
    "    axes[2].set_ylabel('ROUGE-L F1 Score')\n",
    "    axes[2].set_xticklabels(unique_labels, rotation=45, ha='right')\n",
    "    axes[2].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run visualization functions\n",
    "visualize_sequence_predictions(true_sequences, pred_sequences, val_true, val_preds, id_to_label)\n",
    "plot_metric_distributions_by_class(true_sequences, pred_sequences, val_true, id_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Function to test model on specific sequences\n",
    "def test_on_specific_sequences(model, sequences, tokenizer, device, k=6, max_length=512):\n",
    "    \"\"\"\n",
    "    Test the model on specific sequences and return predictions\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    # Process each sequence\n",
    "    for seq in sequences:\n",
    "        # Create k-mers\n",
    "        kmers = [seq[i:i+k] for i in range(len(seq) - k + 1)]\n",
    "        kmer_text = \" \".join(kmers)\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = tokenizer(\n",
    "            kmer_text,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        input_ids = encoding['input_ids'].to(device)\n",
    "        attention_mask = encoding['attention_mask'].to(device)\n",
    "        \n",
    "        # Get prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pred = outputs.argmax(dim=1).item()\n",
    "            predictions.append(pred)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Save complete model evaluation results\n",
    "def save_evaluation_results(val_true, val_preds, true_sequences, pred_sequences, label_names, id_to_label):\n",
    "    \"\"\"\n",
    "    Save detailed evaluation results to file\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"accuracy\": accuracy_score(val_true, val_preds),\n",
    "        \"classification_report\": classification_report(val_true, val_preds, target_names=label_names, output_dict=True),\n",
    "        \"sequence_metrics\": []\n",
    "    }\n",
    "    \n",
    "    # Calculate per-sequence metrics\n",
    "    smooth = SmoothingFunction().method1\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    for i, (true_seq, pred_seq, true_label, pred_label) in enumerate(zip(\n",
    "            true_sequences, pred_sequences, val_true, val_preds)):\n",
    "        \n",
    "        # Get label names\n",
    "        true_label_name = id_to_label[true_label]\n",
    "        pred_label_name = id_to_label[pred_label]\n",
    "        \n",
    "        # Calculate BLEU\n",
    "        try:\n",
    "            true_tokens = list(true_seq)\n",
    "            pred_tokens = list(pred_seq)\n",
    "            bleu = sentence_bleu([true_tokens], pred_tokens, smoothing_function=smooth)\n",
    "        except:\n",
    "            bleu = None\n",
    "        \n",
    "        # Calculate ROUGE\n",
    "        try:\n",
    "            rouge_scores = scorer.score(true_seq, pred_seq)\n",
    "            rouge1 = rouge_scores['rouge1'].fmeasure\n",
    "            rouge2 = rouge_scores['rouge2'].fmeasure\n",
    "            rougeL = rouge_scores['rougeL'].fmeasure\n",
    "        except:\n",
    "            rouge1, rouge2, rougeL = None, None, None\n",
    "        \n",
    "        # Store metrics\n",
    "        results[\"sequence_metrics\"].append({\n",
    "            \"index\": i,\n",
    "            \"true_label\": true_label,\n",
    "            \"true_label_name\": true_label_name,\n",
    "            \"pred_label\": pred_label,\n",
    "            \"pred_label_name\": pred_label_name,\n",
    "            \"correct\": true_label == pred_label,\n",
    "            \"bleu\": bleu,\n",
    "            \"rouge1\": rouge1,\n",
    "            \"rouge2\": rouge2,\n",
    "            \"rougeL\": rougeL,\n",
    "            \"sequence_length\": len(true_seq)\n",
    "        })\n",
    "    \n",
    "    # Save to file\n",
    "    with open('evaluation_results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(\"Evaluation results saved to 'evaluation_results.json'\")\n",
    "\n",
    "# Run final evaluation and save results\n",
    "save_evaluation_results(val_true, val_preds, true_sequences, pred_sequences, unique_labels, id_to_label)\n",
    "\n",
    "print(\"Complete evaluation with BLEU and ROUGE metrics finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
